{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on Aug 9, 2016\n",
    "Updated on May 20, 2018\n",
    "\n",
    "Keras Implementation of Generalized Matrix Factorization (GMF) recommender model in:\n",
    "He Xiangnan et al. Neural Collaborative Filtering. In WWW 2017.  \n",
    "\n",
    "@original author: Xiangnan He (xiangnanhe@gmail.com)\n",
    "@Updated and placed on notebooks: Guy Shtar (shtar@post.bgu.ac.il)\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as T\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Multiply, Concatenate\n",
    "from Dataset import Dataset\n",
    "from evaluate import evaluate_model\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import math\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done [16.7 s]. #user=6040, #item=3706, #train=994169, #test=6040\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "path=r'C:\\Users\\Administrator\\Documents\\neural_collaborative_filtering\\Data\\\\'\n",
    "dataset='ml-1m'\n",
    "#dataset='pinterest-20'\n",
    "t1 = time()\n",
    "dataset = Dataset(path + dataset)\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_GMF_model(num_users, num_items, latent_dim, regs=[[0,0]]):\n",
    "    #Generalized Matrix Factorization\n",
    "    \n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = Multiply()([user_latent, item_latent]) #merge([user_latent, item_latent], mode = 'mul')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_MLP_model(num_users, num_items, latent_dim, regs=[[0,0],0,0], layers = [20,10]):\n",
    "    #Multi-Layer Perceptron\n",
    "    \n",
    "    assert len(layers) + 1 == len(regs), 'the number of regs is equal to number of layers + the embedding layer'\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    \n",
    "    # Concatenation of embedding layers\n",
    "    vector = Concatenate(axis=-1)([user_latent, item_latent])#merge([user_latent, item_latent], mode = 'concat')\n",
    "    \n",
    "    # MLP layers\n",
    "    for idx in range(num_layer):\n",
    "        layer = Dense(layers[idx], kernel_regularizer = l2(regs[idx+1]), activation='relu', name = 'layer%d' %idx)\n",
    "        vector = layer(vector)\n",
    "        \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "def get_NMF_model(num_users, num_items, latent_dim_GMF, latent_dim_MLP, reg_GMF=[[0,0]], regs_MLP=[[0,0],0,0], layers=[20,10]):\n",
    "    #Neural matrix factorization\n",
    "    assert len(layers) + 1 == len(regs_MLP), 'the number of regs is equal to number of layers + the embedding layer'\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim_GMF, name = 'MF_user_embedding',\n",
    "                                   embeddings_regularizer = l2(reg_GMF[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim_GMF, name = 'MF_item_embedding',\n",
    "                                   embeddings_regularizer = l2(reg_GMF[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim_MLP, name = 'MLP_user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs_MLP[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim_MLP, name = 'MLP_item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs_MLP[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) #merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = Concatenate(axis=-1)([mlp_user_latent, mlp_item_latent])#merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "    for idx in range(num_layer):\n",
    "        layer =  Dense(layers[idx], kernel_regularizer = l2(regs_MLP[idx+1]), activation='tanh', name = 'layer%d' %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    predict_vector = Concatenate(axis=-1)([mf_vector, mlp_vector])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = \"prediction\")(predict_vector)    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MLP_user_embedding (Embedding)  (None, 1, 8)         48320       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MLP_item_embedding (Embedding)  (None, 1, 8)         29648       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 8)            0           MLP_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 8)            0           MLP_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16)           0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MF_user_embedding (Embedding)   (None, 1, 8)         48320       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MF_item_embedding (Embedding)   (None, 1, 8)         29648       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer0 (Dense)                  (None, 32)           544         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 8)            0           MF_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 8)            0           MF_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 16)           528         layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 8)            0           flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 8)            136         layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16)           0           multiply_4[0][0]                 \n",
      "                                                                 layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            17          concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 157,161\n",
      "Trainable params: 157,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Init: HR = 0.0940, NDCG = 0.0422\t [13.4 s]\n",
      "Iteration 0 [80.1 s]: HR = 0.5949, NDCG = 0.3344, loss = 0.3203 [10.2 s]\n",
      "Iteration 1 [72.5 s]: HR = 0.6296, NDCG = 0.3657, loss = 0.2772 [12.5 s]\n",
      "Iteration 2 [73.8 s]: HR = 0.6478, NDCG = 0.3750, loss = 0.2680 [12.6 s]\n",
      "Iteration 3 [76.9 s]: HR = 0.6490, NDCG = 0.3809, loss = 0.2641 [12.8 s]\n",
      "Iteration 4 [70.9 s]: HR = 0.6568, NDCG = 0.3863, loss = 0.2610 [10.3 s]\n",
      "Iteration 5 [73.4 s]: HR = 0.6586, NDCG = 0.3869, loss = 0.2590 [11.2 s]\n",
      "Iteration 6 [71.0 s]: HR = 0.6601, NDCG = 0.3870, loss = 0.2573 [12.8 s]\n",
      "Iteration 7 [81.2 s]: HR = 0.6641, NDCG = 0.3903, loss = 0.2558 [12.8 s]\n",
      "Iteration 8 [75.7 s]: HR = 0.6654, NDCG = 0.3914, loss = 0.2547 [12.6 s]\n",
      "Iteration 9 [80.3 s]: HR = 0.6685, NDCG = 0.3959, loss = 0.2535 [12.1 s]\n",
      "End. Best Iteration 9:  HR = 0.6685, NDCG = 0.3959. \n"
     ]
    }
   ],
   "source": [
    "num_factors = 8 #size of embedding size. Can be split to 4 different params potentially.\n",
    "num_negatives = 4 #how many negative samples per positive sample?\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "write_model=False\n",
    "topK = 10 #used to evaluate the model. Top K recommendations are used.\n",
    "evaluation_threads = 1 \n",
    "model_out_file = 'Pretrain/%s_GMF_%d_%d.h5' %(dataset, num_factors, time())\n",
    "\n",
    "# Build model\n",
    "#model = get_GMF_model(num_users, num_items, num_factors, regs = [[0,0]])\n",
    "#model = get_MLP_model(num_users, num_items, num_factors, regs = [[0,0],0,0,0], layers = [32,16,8])\n",
    "model = get_NMF_model(num_users, num_items, latent_dim_GMF=num_factors, latent_dim_MLP=num_factors, reg_GMF=[[0,0]],\n",
    "                      regs_MLP=[[0,0],0,0,0], layers=[32,16,8])\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "\n",
    "# Init performance\n",
    "t1 = time()\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]' % (hr, ndcg, time()-t1))\n",
    "\n",
    "# Train model\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "    # Training\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                     np.array(labels), # labels \n",
    "                     batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "    t2 = time()\n",
    "\n",
    "    # Evaluation\n",
    "    if epoch %verbose == 0:\n",
    "        (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "        print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "              % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "            if write_model:\n",
    "                model.save_weights(model_out_file, overwrite=True)\n",
    "\n",
    "print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "if write_model:\n",
    "    print(\"The best GMF model is saved to %s\" %(model_out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
